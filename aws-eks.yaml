# Define common used anchors
_: &getKubeconfig "rm -f ../kubeconfig_{{ .name }}; aws eks --region {{ .variables.region }} update-kubeconfig --name {{ .name }} --kubeconfig ../kubeconfig_{{ .name }}"
_p: &provider_aws
- aws:
    region: {{ .variables.region }}

{{ $createVpcCIDR := "10.8.0.0/18" -}}
{{- $azs_count := len .variables.azs -}}
{{- $skip_subnet_tagging := .variables.skip_subnets_tagging }}
# Define template
kind: InfraTemplate
name: aws-eks
# Define Modules itself
modules:
  - name: route53
    type: terraform
    source: github.com/shalb/cluster.dev-domain?ref=v0.1.0
    inputs:
      region: {{ .variables.region }}
      cluster_name: {{ .name }}
      cluster_domain: {{ .variables.domain }}
      zone_delegation: {{ if eq .variables.domain "cluster.dev" }}true{{ else }}false{{ end }}

  {{- if not .variables.vpc_id }}
  {{- $skip_subnet_tagging = false }}
  - name: vpc
    type: terraform
    providers: *provider_aws
    source: terraform-aws-modules/vpc/aws
    version: "2.70.0"
    inputs:
      name: {{ .name }}
      cidr: {{ $createVpcCIDR }}
      public_subnets:
      {{- range $index, $_ := .variables.azs }}
        - {{ cidrSubnet $createVpcCIDR 4 $index }}
      {{- end }}
      private_subnets:
      {{- range $index, $_ := .variables.azs }}
        - {{ cidrSubnet $createVpcCIDR 4 (add $index $azs_count ) }}
      {{- end }}
      azs: {{ insertYAML .variables.azs }}
  {{- end }}

  {{- if not $skip_subnet_tagging }}
  - name: subnets_tagging
    type: terraform
    providers: *provider_aws
    source: ./subnets_tagging/
    inputs:
  {{- if not .variables.vpc_id }}
      public_subnets: {{ remoteState "this.vpc.public_subnets" }}
      private_subnets: {{ remoteState "this.vpc.private_subnets" }}
  {{- else }}
      public_subnets: {{ insertYAML .variables.public_subnets }}
  {{- end }}
  {{- end }}  

  - name: eks
    type: terraform
    providers: *provider_aws
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
      on_apply: false
      on_plan: false
    post_hook:
      command: cp ./kubeconfig_{{ .name }} ../kubeconfig_{{ .name }}
  {{- if not $skip_subnet_tagging }}
    depends_on: this.subnets_tagging 
  {{- end }}
    source: "terraform-aws-modules/eks/aws"
    version: "13.2.1"
    inputs:
      cluster_name: {{ .name }}
      enable_irsa: true
      manage_aws_auth: false
      cluster_endpoint_private_access: true
      cluster_endpoint_public_access: true
      cluster_version: {{ .variables.eks_version }}
      {{- if not .variables.vpc_id }}
      subnets: {{ remoteState "this.vpc.public_subnets" }}
      vpc_id: {{ remoteState "this.vpc.vpc_id" }}
      {{- else }}
      subnets: {{ insertYAML .variables.public_subnets }}
      vpc_id: {{ .variables.vpc_id }}
      {{- end }}
      worker_groups_launch_template: {{ insertYAML .variables.default_node_pool }}

  - name: eks_auth
    type: kubernetes
    provider_version: "0.2.1"
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.eks
    source: ./eks/

  - name: cert-manager
    type: helm
    source:
      repository: "https://charts.jetstack.io"
      chart: "cert-manager"
      version: "v1.2.0"
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.eks_auth
    additional_options:
      namespace: "cert-manager"
      create_namespace: true
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
      on_plan: true
    inputs:
      installCRDs: true
      webhook.enabled: false
      ingressShim.defaultIssuerName: letsencrypt-prod
      ingressShim.defaultIssuerKind: ClusterIssuer
      ingressShim.defaultACMEChallengeType: dns01
      securityContext.enabled: false
      serviceAccount.create: true
      serviceAccount.annotations.eks\.amazonaws\.com/role-arn: {{ remoteState "this.iam_assumable_role_route53.this_iam_role_arn" }}

  - name: cert-manager-issuer
    type: kubernetes
    source: ./cert-manager/
    provider_version: "0.2.1"
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.cert-manager
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
      on_plan: true

  - name: ingress-nginx
    type: helm
    source:
      repository: "https://kubernetes.github.io/ingress-nginx"
      chart: "ingress-nginx"
      version: "3.21.0"
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.eks_auth
    additional_options:
      namespace: "ingress-nginx"
      create_namespace: true
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
      on_plan: true
    inputs:
        service.type: LoadBalancer
        controller.admissionWebhooks.enabled: false

  - name: argocd
    type: helm
    source:
      repository: "https://argoproj.github.io/argo-helm"
      chart: "argo-cd"
      version: "2.17.5"
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.cert-manager
    additional_options:
      namespace: "argocd"
      create_namespace: true
    inputs:
      global.image.tag: v1.8.4
      service.type: LoadBalancer
      server.certificate.enabled: true
      server.certificate.domain: argocd.{{ .name }}.{{ .variables.domain }}
      server.certificate.issuer.name: letsencrypt-prod
      server.certificate.issuer.kind: ClusterIssuer
      server.ingress.enabled: true
      server.ingress.annotations.nginx\.ingress\.kubernetes\.io/ssl-passthrough: true
      server.ingress.annotations.nginx\.ingress\.kubernetes\.io/backend-protocol: HTTPS
      server.ingress.hosts[0]: argocd.{{ .name }}.{{ .variables.domain }}
      server.ingress.tls[0].hosts[0]: argocd.{{ .name }}.{{ .variables.domain }}
      server.ingress.tls[0].secretName: argocd-secret
      server.config.url: https://argocd.{{ .name }}.{{ .variables.domain }}
      configs.secret.argocdServerAdminPassword: {{ .variables.argocdServerAdminPassword }}

  - name: argocd_apps
    type: kubernetes
    provider_version: "0.2.1"
    source: ./argocd-apps/
    pre_hook:
      command: *getKubeconfig
      on_destroy: true
    kubeconfig: ../kubeconfig_{{ .name }}
    depends_on: this.argocd

  - name: iam_assumable_role_route53
    type: terraform
    source: "terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc"
    version: "~> 3.0"
    providers: *provider_aws
    inputs:
      role_name: "eks-route53-{{ remoteState "this.eks.cluster_id" }}"
      create_role: true
      role_policy_arns:
        - {{ remoteState "this.iam_policy_route53.arn" }}
      oidc_fully_qualified_subjects:
        - "system:serviceaccount:external-dns:external-dns"
        - "system:serviceaccount:cert-manager:cert-manager"
      provider_url: {{ remoteState "this.eks.cluster_oidc_issuer_url" | replace "https://" ""}}

  - name: iam_policy_route53
    type: terraform
    source: "terraform-aws-modules/iam/aws//modules/iam-policy"
    version: "~> 3.0"
    providers: *provider_aws
    inputs:
      name: AllowExternalDNSUpdates-{{ .name }}
      policy: |
        {
          "Version": "2012-10-17",
          "Statement": [
            {
              "Effect": "Allow",
              "Action": [
                "route53:ChangeResourceRecordSets",
                "route53:GetChange"
              ],
              "Resource": [
                "arn:aws:route53:::hostedzone/{{ remoteState "this.route53.zone_id" }}",
                "arn:aws:route53:::change/*"
              ]
            },
            {
              "Effect": "Allow",
              "Action": [
                "route53:ListHostedZones",
                "route53:ListResourceRecordSets",
                "route53:ListHostedZonesByName"
              ],
              "Resource": [
                "*"
              ]
            }
          ]
        }

  - name: print_outputs
    type: printer
    depends_on: this.argocd_apps
    inputs:
      cluster_name: {{ .name }}
      kubeconfig: rm -f kubeconfig_{{ .name }}; aws eks --region {{ .variables.region }} update-kubeconfig --name {{ .name }} --kubeconfig kubeconfig_{{ .name }} && export KUBECONFIG=kubeconfig_{{ .name }}
